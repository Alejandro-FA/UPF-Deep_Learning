# Deep Learning Practice 2 report document

*RNNs and LSTMs*

Group 8

Andreu Garcies (240618), Alejandro Fern√°ndez (242349), Marc Aguilar (242192)

# Exercise 1

## Train a RNN or LSTM to solve the multi-class sequence classification problem.

### How have we adapted the classification loss and the SequenceClassifier module
In order to use the `SequenceClassifier` to solve our problem we modified the last linear layer and the activation function. 

First of all, after analyzing the shape of the data and the goal of our network we realized that in the last layer we needed at least as many neurons as classes we wanted to distinguish. Therefore, we added the parameter `n_classes` to both the class and the layer (`nn.Linear(hidden_size, n_classes)`). 

After the first modification and taking into account that we are solving a multiclass classification problem, the activation function which fitted the best was a **Softmax**, which directly returns probability that the sequence belongs to each of the classes. 

### How have we adapted the test_sequence_classifier function to compute the multi-class accuracy

For the confusion matrix we have decided to use the `sklearn.metrics`, in particular the `confusion_matrix` and `ConfusionMatrixDisplay` modules. We use the first module to compute the values of the confusion matrix and the second one to plot the results. We have not modified the `test_sequence_classifier` function in any other meaningful way. One minor detail to remark is that any operation related to confusion matrices cannot be performed in the GPU.


## Experiment with different models by changing different hyper-parameters (e.g, num_layers, hidden_size, optimiziers, activation_functions for RNNs, etc..) and evaluate the results for each of them on the testing set.

In order to search for the best performance possible we have tried multiple hyperparameter combinations, as well as some other alternatives like using bidirectional LSTMs and weight decay. Here we detail our exploration of several parameters and the results found:

- **Bias**: Using a bias parameter in the model consistently hurts performance, so it was rapidly discarded as a good option.

- **Optimizer used**: For this model we tried multiple optimizers from those available in `pytorch` (in fact, we tried all of them). Although we did not spend time fine-tuning each of the optimizers, we 

# Exercise 2



